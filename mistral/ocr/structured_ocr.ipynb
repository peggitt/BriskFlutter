{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPiAIwHteCl"
      },
      "source": [
        "# OCR Cookbook\n",
        "\n",
        "---\n",
        "\n",
        "## OCR Exploration and Structured Outputs\n",
        "In this cookbook, we will explore the basics of OCR and leverage it together with existing models to achieve structured outputs fueled by our OCR model.\n",
        "\n",
        "You may want to do this in case current vision models are not powerful enough, hence enhancing their vision OCR capabilities with the OCR model to achieve better structured data extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### Used\n",
        "- Mistral OCR\n",
        "- Pixtral 12B & Ministral 8B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgZW4ZfetwAl"
      },
      "source": [
        "### Setup\n",
        "First, let's install `mistralai` and download the required files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po7Cukllt8za"
      },
      "outputs": [],
      "source": [
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKgrASwF3Ol"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/mistral7b.pdf\n",
        "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwM0aITt7ti"
      },
      "source": [
        "We can now set up our client. You can create an API key on our [Plateforme](https://console.mistral.ai/api-keys/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odfkuCk6qSAw"
      },
      "outputs": [],
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"API_KEY\"\n",
        "client = Mistral(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-3YwljuFKK"
      },
      "source": [
        "There are two types of files you can apply OCR to:\n",
        "- PDF files, either uploaded or from URLs..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iJ9TdZBrfJ2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "pdf_file = Path(\"mistral7b.pdf\")\n",
        "assert pdf_file.is_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svaJGBFlqm7_"
      },
      "outputs": [],
      "source": [
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "import json\n",
        "\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "pdf_response = client.ocr.process(document=DocumentURLChunk(document_url=signed_url.url), model=\"mistral-ocr-latest\", include_image_base64=True)\n",
        "\n",
        "response_dict = json.loads(pdf_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG2_TdlKIxYs"
      },
      "source": [
        "*The OCR model can output interleaved text and images (set `include_image_base64=True` to return the base64 image ), we can view the result with the following:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxefUpm-Idp8"
      },
      "outputs": [],
      "source": [
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        markdown_str = markdown_str.replace(f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\")\n",
        "    return markdown_str\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "  markdowns: list[str] = []\n",
        "  for page in ocr_response.pages:\n",
        "    image_data = {}\n",
        "    for img in page.images:\n",
        "      image_data[img.id] = img.image_base64\n",
        "    markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "  return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "display(Markdown(get_combined_markdown(pdf_response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yk5tBpPuKal"
      },
      "source": [
        "- And Image files..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vZ8gnTRrejO"
      },
      "outputs": [],
      "source": [
        "image_file = Path(\"receipt.png\")\n",
        "assert image_file.is_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFdyKIcgrahm"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "encoded = base64.b64encode(image_file.read_bytes()).decode()\n",
        "base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
        "\n",
        "image_response = client.ocr.process(document=ImageURLChunk(image_url=base64_data_url), model=\"mistral-ocr-latest\")\n",
        "\n",
        "response_dict = json.loads(image_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWStbt7LuMvT"
      },
      "source": [
        "We want to be able to extract structured data from these files. For this, we will make use of `pixtral-12b-latest` and support it with our OCR model for better, high-quality answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aZOQs38r0GO"
      },
      "outputs": [],
      "source": [
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"pixtral-12b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                ImageURLChunk(image_url=base64_data_url),\n",
        "                TextChunk(text=f\"This is image's OCR in markdown:\\n<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\nConvert this into a sensible structured json response. The output should be strictly be json with no extra commentary\")\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "    response_format =  {\"type\": \"json_object\"},\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YKioib1vgTZ"
      },
      "source": [
        "Note: We are leveraging a model already capable of vision tasks. However, we could also use text-only models for the structured output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH2-OlXqvqTT"
      },
      "outputs": [],
      "source": [
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"ministral-8b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"This is image's OCR in markdown:\\n<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\nConvert this into a sensible structured json response. The output should be strictly be json with no extra commentary\"\n",
        "        },\n",
        "    ],\n",
        "    response_format =  {\"type\": \"json_object\"},\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc__PKmkwUnZ"
      },
      "source": [
        "### All Together\n",
        "Let's design a simple function that takes an `image_path` file and returns a JSON structured output in a specific format. In this case, we arbitrarily decided we wanted an output respecting the following:\n",
        "\n",
        "```python\n",
        "class StructuredOCR:\n",
        "    file_name: str  # can be any string\n",
        "    topics: list[str]  # must be a list of strings\n",
        "    languages: list[Language]  # a list of languages\n",
        "    ocr_contents: dict  # any dictionary, can be freely defined by the model\n",
        "```\n",
        "\n",
        "We will make use of [custom structured outputs](https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/) as well as `pycountry` for the languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsKWV5pyxp8u"
      },
      "outputs": [],
      "source": [
        "!pip install pycountry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM2ensmIwh4H"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "import pycountry\n",
        "\n",
        "languages = {lang.alpha_2: lang.name for lang in pycountry.languages if hasattr(lang, 'alpha_2')}\n",
        "\n",
        "class LanguageMeta(Enum.__class__):\n",
        "    def __new__(metacls, cls, bases, classdict):\n",
        "        for code, name in languages.items():\n",
        "            classdict[name.upper().replace(' ', '_')] = name\n",
        "        return super().__new__(metacls, cls, bases, classdict)\n",
        "\n",
        "class Language(Enum, metaclass=LanguageMeta):\n",
        "    pass\n",
        "\n",
        "class StructuredOCR(BaseModel):\n",
        "    file_name: str\n",
        "    topics: list[str]\n",
        "    languages: list[Language]\n",
        "    ocr_contents: dict\n",
        "\n",
        "print(StructuredOCR.schema_json())\n",
        "\n",
        "def structured_ocr(image_path: str) -> StructuredOCR:\n",
        "    image_file = Path(image_path)\n",
        "    assert image_file.is_file(), \"The provided image path does not exist.\"\n",
        "\n",
        "    # Read and encode the image file\n",
        "    encoded_image = base64.b64encode(image_file.read_bytes()).decode()\n",
        "    base64_data_url = f\"data:image/jpeg;base64,{encoded_image}\"\n",
        "\n",
        "    # Process the image using OCR\n",
        "    image_response = client.ocr.process(document=ImageURLChunk(image_url=base64_data_url), model=\"mistral-ocr-latest\")\n",
        "    image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "    # Parse the OCR result into a structured JSON response\n",
        "    chat_response = client.chat.parse(\n",
        "        model=\"pixtral-12b-latest\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    ImageURLChunk(image_url=base64_data_url),\n",
        "                    TextChunk(text=(\n",
        "                        \"This is the image's OCR in markdown:\\n\"\n",
        "                        f\"<BEGIN_IMAGE_OCR>\\n{image_ocr_markdown}\\n<END_IMAGE_OCR>.\\n\"\n",
        "                        \"Convert this into a structured JSON response with the OCR contents in a sensible dictionnary.\"\n",
        "                    ))\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        response_format=StructuredOCR,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return chat_response.choices[0].message.parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVipACEOAyEX"
      },
      "source": [
        "We can now extract structured output from any image parsed with our OCR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvt3OAcpyXCF"
      },
      "outputs": [],
      "source": [
        "image_path = \"receipt.png\"\n",
        "structured_response = structured_ocr(image_path)\n",
        "\n",
        "response_dict = json.loads(structured_response.json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8705WaqA8KV"
      },
      "source": [
        "The original image for comparison can be found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xj9tOTKA7mw"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}